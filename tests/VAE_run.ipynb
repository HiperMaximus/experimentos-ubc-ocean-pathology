{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "import os\n",
    "import gc\n",
    "\n",
    "os.environ['TORCH_LOGS'] = \"+dynamo\"\n",
    "os.environ['TORCHDYNAMO_VERBOSE'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1),  # Output: (32, 64, 64)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), # Output: (64, 32, 32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), # Output: (128, 16, 16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1), # Output: (256, 8, 8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(256, 512, 3, stride=2, padding=1), # Output: (512, 4, 4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(4 * 4 * 512, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(4 * 4 * 512, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim, 4 * 4 * 512)\n",
    "\n",
    "        # Decoder using ConvTranspose2d for upsampling\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            # Reshape from latent vector to feature map\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),  # Output: (256, 8, 8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # Output: (128, 16, 16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # Output: (64, 32, 32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # Output: (32, 64, 64)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),    # Output: (16, 128, 128)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            # Final layer: project to 3 channels (RGB) with 3x3 convolution\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1),  # Output: (3, 128, 128)\n",
    "            nn.Tanh()  # Or Sigmoid depending on normalization\n",
    "            # Final layer outputs RGB values between -1 and 1\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def encode(self,x):\n",
    "        # Encode\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def decode(self,z):\n",
    "        # Decode\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, 512, 4, 4)  # Reshape to image feature map\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def vae_loss(self, recon_x, x, mu, logvar, gamma=5e-5):\n",
    "        # Reconstruction loss (MSE or L1 loss)\n",
    "        recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "        \n",
    "        # KL Divergence loss\n",
    "        kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        \n",
    "        return recon_loss + (gamma*kld_loss)\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        mu, logvar =self.encode(x)\n",
    "\n",
    "        # Reparameterize\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        \n",
    "        # Decode\n",
    "        x=self.decode(z)\n",
    "        return x, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 16, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "class ManualConvTranspose2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding='same',bias=True):\n",
    "        super(ManualConvTranspose2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Convolution layer with padding=0 because we're handling padding manually\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding,bias=bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Step 1: Zero-insert to upsample\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        \n",
    "        # Calculate upsampled dimensions\n",
    "        upsampled_height = height * self.stride\n",
    "        upsampled_width = width * self.stride\n",
    "        upsampled = torch.zeros(batch_size, channels, upsampled_height, upsampled_width, device=x.device)\n",
    "        \n",
    "        # Step 2: Fill in original values, leaving zero-inserted spaces\n",
    "        upsampled[:, :, ::self.stride, ::self.stride] = x\n",
    "\n",
    "        # Step 3: Apply convolution\n",
    "        output = self.conv(upsampled)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Testing the implementation\n",
    "input_tensor = torch.randn(1, 16, 4, 4)  # Example input\n",
    "model = ManualConvTranspose2d(in_channels=16, out_channels=16, kernel_size=4, stride=2, padding='same')\n",
    "output = model(input_tensor)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "# Model, optimizer, and compilation\n",
    "latent_dim = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TiledImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, image_ids, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_ids = image_ids\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        \n",
    "        # Collect all image paths for the given image IDs\n",
    "        for image_id in image_ids:\n",
    "            image_folder = os.path.join(data_dir, image_id)\n",
    "            for img_name in os.listdir(image_folder):\n",
    "                self.image_paths.append(os.path.join(image_folder, img_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `image_ids` is a list of all folder names (IDs of the original images)\n",
    "data_dir = 'dataset'\n",
    "image_ids = os.listdir(data_dir)  # List of all original image IDs\n",
    "\n",
    "# Split image IDs\n",
    "train_ids, temp_ids = train_test_split(image_ids, test_size=0.3, random_state=42)\n",
    "val_ids, test_ids = train_test_split(temp_ids, test_size=2/3, random_state=42)  # 10% validation, 20% test\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TiledImageDataset(data_dir, train_ids, transform=transform)\n",
    "val_dataset = TiledImageDataset(data_dir, val_ids, transform=transform)\n",
    "test_dataset = TiledImageDataset(data_dir, test_ids, transform=transform)\n",
    "\n",
    "batch_size=2**9\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10% of training data for each epoch\n",
    "def get_subset_sampler(dataset, percentage=0.1):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    split = int(np.ceil(percentage * dataset_size))\n",
    "    train_indices = indices[:split]\n",
    "    \n",
    "    return SubsetRandomSampler(train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, optimizer, and compilation\n",
    "latent_dim = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = VAE(latent_dim)\n",
    "compiled_vae = torch.compile(vae).to(device)\n",
    "#compiled_vae.load_state_dict(torch.load('epochs/epoch_10/vae_weights_epoch_10.pth',weights_only=True))\n",
    "\n",
    "optimizer = optim.AdamW(compiled_vae.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min', \n",
    "    factor=0.9, \n",
    "    patience=1,\n",
    "    cooldown=1)\n",
    "\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Select and save 24 images from the test set (ensure to keep this consistent across epochs)\n",
    "n=24\n",
    "fixed_batch = next(iter(DataLoader(test_dataset, batch_size=n, shuffle=False)))\n",
    "for i in range(n):\n",
    "    save_image(fixed_batch[i],fp=f'epochs/img_{i}.png')\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir='runs/vae_experiment')\n",
    "# Log original images to TensorBoard\n",
    "writer.add_images('Original Images', fixed_batch, 0)\n",
    "\n",
    "\n",
    "# sample 10% of data per epoch and log everything\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    #sampler = get_subset_sampler(train_dataset, percentage=0.1)\n",
    "    #train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    accumulated_gradients = {name: torch.zeros_like(param) for name, param in compiled_vae.named_parameters()}\n",
    "    compiled_vae.train()\n",
    "    train_loss = 0\n",
    "    num_samples=0\n",
    "    # Use tqdm to create a progress bar for the training loop\n",
    "    with tqdm(total=len(train_loader), desc=f'Training Epoch {epoch + 1}/{num_epochs}', unit='batch', position=0, leave=False) as pbar:\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            recon_batch, mu, logvar = compiled_vae(batch)\n",
    "            loss = compiled_vae.vae_loss(recon_batch, batch, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            for name, param in compiled_vae.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    accumulated_gradients[name] += param.grad.detach().clone()\n",
    "\n",
    "            num_samples+=batch.size()[0]\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)  # Increment the progress bar by 1\n",
    "            pbar.set_postfix({'train_loss': loss.item()})  # Display current loss\n",
    "\n",
    "\n",
    "    train_loss /= num_samples\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "\n",
    "    # Create current directory if doesn't exists\n",
    "    current_dir=f'epochs/epoch_{epoch +1}/'\n",
    "    os.makedirs(current_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Validation loop\n",
    "    compiled_vae.eval()\n",
    "    val_loss = 0\n",
    "    num_samples_val=0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(val_loader), desc=f'Validation Epoch {epoch + 1}/{num_epochs}', unit='batch',position=1, leave=False) as pbar:\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                recon_batch, mu, logvar = compiled_vae(batch)\n",
    "                loss = compiled_vae.vae_loss(recon_batch, batch, mu, logvar)\n",
    "                val_loss += loss.item()\n",
    "                num_samples_val+=batch.size()[0]\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.update(1)  # Increment the progress bar by 1\n",
    "                pbar.set_postfix({'val_loss': loss.item()})  # Display current loss\n",
    "\n",
    "    val_loss /= num_samples_val\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "    # Update the learning rate scheduler based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    # The learning rate in optimizer will be updated automatically if needed\n",
    "    current_lr = scheduler.get_last_lr()[0]  # To check the current learning rate\n",
    "\n",
    "    # At the end of each epoch, save the reconstructions\n",
    "    compiled_vae.eval()\n",
    "    with torch.no_grad():\n",
    "        fixed_batch_epoch = fixed_batch.to(device)\n",
    "        mu, _ = compiled_vae.encode(fixed_batch_epoch)\n",
    "        recon_batch = compiled_vae.decode(mu)\n",
    "        recon_batch=recon_batch.cpu()\n",
    "\n",
    "        # Reverse normalization\n",
    "        recon_batch = (recon_batch*0.5)+0.5 \n",
    "        # Clamp the values to be in the range [0, 1]\n",
    "        recon_batch = torch.clamp(recon_batch, 0, 1)\n",
    "        for i in range(n):\n",
    "            save_image(recon_batch[i],fp=os.path.join(current_dir,f'img_recon_{i}.png'))\n",
    "    \n",
    "    # Log reconstructed images to TensorBoard\n",
    "    writer.add_images('Reconstructed Images', recon_batch, epoch)\n",
    "\n",
    "    # Log the model parameters and accumulated gradients to TensorBoard after the epoch\n",
    "    avg_gradients=copy.deepcopy(accumulated_gradients)\n",
    "    for name, param in compiled_vae.named_parameters():\n",
    "        writer.add_histogram(f'Weights/{name}', param, global_step=epoch)\n",
    "        if param.grad is not None:\n",
    "            # Log averaged gradients\n",
    "            avg_gradient = accumulated_gradients[name] / num_samples\n",
    "            avg_gradients[name]=avg_gradient\n",
    "            writer.add_histogram(f'Gradients/{name}', avg_gradient, global_step=epoch)\n",
    "\n",
    "    # Save both model and optimizer states\n",
    "    torch.save({\n",
    "        'epoch': epoch+1,\n",
    "        'model_state_dict': compiled_vae.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss':val_loss,\n",
    "        'num_samples': num_samples,\n",
    "        'avg_gradients':avg_gradients\n",
    "    }, os.path.join(current_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "\n",
    "    # Save best model when validation loss improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "        'epoch': epoch+1,\n",
    "        'model_state_dict': compiled_vae.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss':val_loss,\n",
    "        'num_samples': num_samples,\n",
    "        'avg_gradients':avg_gradients\n",
    "        }, os.path.join('epochs/best_model/', 'vae_best_model.pth'))\n",
    "\n",
    "    # Log training and validation losses\n",
    "    writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "\n",
    "    tqdm.write(f'Epoch {epoch + 1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Learning Rate: {current_lr}')\n",
    "\n",
    "\n",
    "# After training loop\n",
    "\n",
    "# Calculate test loss\n",
    "compiled_vae.eval()\n",
    "test_loss = 0\n",
    "num_samples = 0\n",
    "with tqdm(total=len(val_loader), desc=f'Validation', unit='batch') as pbar:\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            recon_batch, mu, logvar = compiled_vae(batch)\n",
    "            loss = compiled_vae.vae_loss(recon_batch, batch, mu, logvar)\n",
    "            test_loss += loss.item()\n",
    "            num_samples += batch.size()[0]\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)  # Increment the progress bar by 1\n",
    "            pbar.set_postfix({'val_loss': loss.item()})  # Display current loss\n",
    "test_loss /= num_samples\n",
    "\n",
    "# Log test losses\n",
    "writer.add_scalar('Loss/Test', test_loss, epoch)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "# Print test loss\n",
    "tqdm.write(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('loss_plot.png')  # Save the plot as an image file\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
