{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_image\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SubsetRandomSampler\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_image\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvae_equivariant_architecture\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m D4_Equivariant_VAE\n",
      "File \u001b[0;32m/media/max/f7de66f7-119c-4593-a8ab-02f75c636771/Max/experimentos-ubc-ocean-pathology/venv/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_vendor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensorboard, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m Version(\n\u001b[1;32m      5\u001b[0m     tensorboard\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m      6\u001b[0m ) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.15\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import save_image\n",
    "from vae_equivariant_architecture import D4_Equivariant_VAE\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "import os\n",
    "import gc\n",
    "\n",
    "os.environ['TORCH_LOGS'] = \"+dynamo\"\n",
    "os.environ['TORCHDYNAMO_VERBOSE'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TiledImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, image_ids, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_ids = image_ids\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        \n",
    "        # Collect all image paths for the given image IDs\n",
    "        for image_id in image_ids:\n",
    "            image_folder = os.path.join(data_dir, image_id)\n",
    "            for img_name in os.listdir(image_folder):\n",
    "                self.image_paths.append(os.path.join(image_folder, img_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `image_ids` is a list of all folder names (IDs of the original images)\n",
    "data_dir = 'dataset'\n",
    "image_ids = os.listdir(data_dir)  # List of all original image IDs\n",
    "\n",
    "# Split image IDs\n",
    "train_ids, temp_ids = train_test_split(image_ids, test_size=0.3, random_state=42)\n",
    "val_ids, test_ids = train_test_split(temp_ids, test_size=2/3, random_state=42)  # 10% validation, 20% test\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TiledImageDataset(data_dir, train_ids, transform=transform)\n",
    "val_dataset = TiledImageDataset(data_dir, val_ids, transform=transform)\n",
    "test_dataset = TiledImageDataset(data_dir, test_ids, transform=transform)\n",
    "\n",
    "batch_size=2**9current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10% of training data for each epoch\n",
    "def get_subset_sampler(dataset, percentage=0.1):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    split = int(np.ceil(percentage * dataset_size))\n",
    "    train_indices = indices[:split]\n",
    "    \n",
    "    return SubsetRandomSampler(train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, optimizer, and compilation\n",
    "latent_dim = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = D4_Equivariant_VAE(latent_dim)\n",
    "compiled_vae = torch.compile(vae).to(device)\n",
    "#compiled_vae.load_state_dict(torch.load('epochs/epoch_10/vae_weights_epoch_10.pth',weights_only=True))\n",
    "\n",
    "optimizer = optim.AdamW(compiled_vae.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min', \n",
    "    factor=0.9, \n",
    "    patience=1,\n",
    "    cooldown=1)\n",
    "\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Select and save 24 images from the test set (ensure to keep this consistent across epochs)\n",
    "n=24\n",
    "fixed_batch = next(iter(DataLoader(test_dataset, batch_size=n, shuffle=False)))\n",
    "for i in range(n):\n",
    "    save_image(fixed_batch[i],fp=f'epochs/img_{i}.png')\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir='runs/vae_experiment')\n",
    "# Log original images to TensorBoard\n",
    "writer.add_images('Original Images', fixed_batch, 0)\n",
    "\n",
    "\n",
    "# sample 10% of data per epoch and log everything\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    #sampler = get_subset_sampler(train_dataset, percentage=0.1)\n",
    "    #train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    accumulated_gradients = {name: torch.zeros_like(param) for name, param in compiled_vae.named_parameters()}\n",
    "    compiled_vae.train()\n",
    "    train_loss = 0\n",
    "    num_samples=0\n",
    "    # Use tqdm to create a progress bar for the training loop\n",
    "    with tqdm(total=len(train_loader), desc=f'Training Epoch {epoch + 1}/{num_epochs}', unit='batch', position=0, leave=False) as pbar:\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            recon_batch, mu, logvar = compiled_vae(batch)\n",
    "            loss = compiled_vae.vae_loss(recon_batch, batch, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            for name, param in compiled_vae.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    accumulated_gradients[name] += param.grad.detach().clone()\n",
    "\n",
    "            num_samples+=batch.size()[0]\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)  # Increment the progress bar by 1\n",
    "            pbar.set_postfix({'train_loss': loss.item()})  # Display current loss\n",
    "\n",
    "\n",
    "    train_loss /= num_samples\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "\n",
    "    # Create current directory if doesn't exists\n",
    "    current_dir=f'epochs/epoch_{epoch +1}/'\n",
    "    os.makedirs(current_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Validation loop\n",
    "    compiled_vae.eval()\n",
    "    val_loss = 0\n",
    "    num_samples_val=0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(val_loader), desc=f'Validation Epoch {epoch + 1}/{num_epochs}', unit='batch',position=1, leave=False) as pbar:\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                recon_batch, mu, logvar = compiled_vae(batch)\n",
    "                loss = compiled_vae.vae_loss(recon_batch, batch, mu, logvar)\n",
    "                val_loss += loss.item()\n",
    "                num_samples_val+=batch.size()[0]\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.update(1)  # Increment the progress bar by 1\n",
    "                pbar.set_postfix({'val_loss': loss.item()})  # Display current loss\n",
    "\n",
    "    val_loss /= num_samples_val\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "    # Update the learning rate scheduler based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    # The learning rate in optimizer will be updated automatically if needed\n",
    "    current_lr = scheduler.get_last_lr()[0]  # To check the current learning rate\n",
    "\n",
    "    # At the end of each epoch, save the reconstructions\n",
    "    compiled_vae.eval()\n",
    "    with torch.no_grad():\n",
    "        fixed_batch_epoch = fixed_batch.to(device)\n",
    "        mu, _ = compiled_vae.encode(fixed_batch_epoch)\n",
    "        recon_batch = compiled_vae.decode(mu)\n",
    "        recon_batch=recon_batch.cpu()\n",
    "\n",
    "        # Reverse normalization\n",
    "        recon_batch = (recon_batch*0.5)+0.5 \n",
    "        # Clamp the values to be in the range [0, 1]\n",
    "        recon_batch = torch.clamp(recon_batch, 0, 1)\n",
    "        for i in range(n):\n",
    "            save_image(recon_batch[i],fp=os.path.join(current_dir,f'img_recon_{i}.png'))\n",
    "    \n",
    "    # Log reconstructed images to TensorBoard\n",
    "    writer.add_images('Reconstructed Images', recon_batch, epoch)\n",
    "\n",
    "    # Log the model parameters and accumulated gradients to TensorBoard after the epoch\n",
    "    avg_gradients=copy.deepcopy(accumulated_gradients)\n",
    "    for name, param in compiled_vae.named_parameters():\n",
    "        writer.add_histogram(f'Weights/{name}', param, global_step=epoch)\n",
    "        if param.grad is not None:\n",
    "            # Log averaged gradients\n",
    "            avg_gradient = accumulated_gradients[name] / num_samples\n",
    "            avg_gradients[name]=avg_gradient\n",
    "            writer.add_histogram(f'Gradients/{name}', avg_gradient, global_step=epoch)\n",
    "\n",
    "    # Save both model and optimizer states\n",
    "    torch.save({\n",
    "        'epoch': epoch+1,\n",
    "        'model_state_dict': compiled_vae.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss':val_loss,\n",
    "        'num_samples': num_samples,\n",
    "        'avg_gradients':avg_gradients\n",
    "    }, os.path.join(current_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "\n",
    "    # Save best model when validation loss improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "        'epoch': epoch+1,\n",
    "        'model_state_dict': compiled_vae.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss':val_loss,\n",
    "        'num_samples': num_samples,\n",
    "        'avg_gradients':avg_gradients\n",
    "        }, os.path.join('epochs/best_model/', 'vae_best_model.pth'))\n",
    "\n",
    "    # Log training and validation losses\n",
    "    writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "\n",
    "    tqdm.write(f'Epoch {epoch + 1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Learning Rate: {current_lr}')\n",
    "\n",
    "\n",
    "# After training loop\n",
    "\n",
    "# Calculate test loss\n",
    "compiled_vae.eval()\n",
    "test_loss = 0\n",
    "num_samples = 0\n",
    "with tqdm(total=len(val_loader), desc=f'Validation', unit='batch') as pbar:\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            recon_batch, mu, logvar = compiled_vae(batch)\n",
    "            loss = compiled_vae.vae_loss(recon_batch, batch, mu, logvar)\n",
    "            test_loss += loss.item()\n",
    "            num_samples += batch.size()[0]\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)  # Increment the progress bar by 1\n",
    "            pbar.set_postfix({'val_loss': loss.item()})  # Display current loss\n",
    "test_loss /= num_samples\n",
    "\n",
    "# Log test losses\n",
    "writer.add_scalar('Loss/Test', test_loss, epoch)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "# Print test loss\n",
    "tqdm.write(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('loss_plot.png')  # Save the plot as an image file\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
