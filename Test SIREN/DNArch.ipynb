{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typeguard import CollectionCheckStrategy\n",
    "import typeguard\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.fft\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "typeguard.config.collection_check_strategy = CollectionCheckStrategy.ALL_ITEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SIREN Layer using a 1x1 convolution ---\n",
    "class SineLayerConv2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        omega_0: float = np.pi * 10,\n",
    "        is_first: bool = False,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: Number of input channels (e.g., 2 for x,y coordinates)\n",
    "            out_channels: Number of output channels\n",
    "            omega_0: Frequency scaling factor (use a higher value for the first layer)\n",
    "            is_first: Whether this is the first layer in the network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # For the first layer, we use the provided omega_0; for later layers, you don't have to add an omega so we set it equal to 1\n",
    "        self.omega_0 = omega_0 if is_first else 1\n",
    "\n",
    "        # 1x1 convolution applies the same linear transform at each pixel location\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
    "\n",
    "        # Initialize weights according to SIREN paper recommendations\n",
    "        with torch.no_grad():\n",
    "            if is_first:\n",
    "                self.conv.weight.uniform_(-1 / in_channels, 1 / in_channels)\n",
    "            else:\n",
    "                # If you want to allow different omega_0 for hidden layers, divide the bound by omega_0 by changing the self.omega_0 parameter initialization.\n",
    "                bound = np.sqrt(6 / in_channels) / self.omega_0\n",
    "                self.conv.weight.uniform_(-bound, bound)\n",
    "            if self.conv.bias is not None:\n",
    "                self.conv.bias.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.omega_0 * self.conv(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Complete SIREN Network ---\n",
    "class SirenConvNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: int,\n",
    "        out_channels: int,\n",
    "        num_hidden_layers: int,\n",
    "        first_omega_0: float = np.pi * 10,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A SIREN network built with pointwise (1x1) convolutions.\n",
    "\n",
    "        Args:\n",
    "            in_channels: Number of input channels (for coordinates, usually 2 for a (x,y) grid)\n",
    "            hidden_channels: Number of channels in hidden layers\n",
    "            out_channels: Number of output channels (e.g., 3 for an RGB image)\n",
    "            num_hidden_layers: Number of hidden Sine layers\n",
    "            first_omega_0: The omega_0 for the first layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers: list[nn.Module] = []\n",
    "        # First layer with a higher omega_0\n",
    "        layers.append(\n",
    "            SineLayerConv2D(\n",
    "                in_channels, hidden_channels, omega_0=first_omega_0, is_first=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_hidden_layers):\n",
    "            layers.append(\n",
    "                SineLayerConv2D(\n",
    "                    hidden_channels, hidden_channels, omega_0=1, is_first=False\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Final layer: use a plain 1x1 conv without sine activation.\n",
    "        final_layer = nn.Conv2d(hidden_channels, out_channels, kernel_size=1, bias=True)\n",
    "        layers.append(final_layer)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        # Since the pixel range is from -1 to 1, data_range should be 2.0, or the tuple (-1,1).\n",
    "        self.ssim_metric = StructuralSimilarityIndexMeasure(data_range=(-1.0, 1.0))\n",
    "\n",
    "    def loss(self, pred: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the combined loss as the sum of MSE loss and (1 - SSIM).\n",
    "\n",
    "        Args:\n",
    "            pred (torch.Tensor): The predicted image tensor.\n",
    "            x (torch.Tensor): The ground truth image tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A scalar tensor representing the combined loss.\n",
    "        \"\"\"\n",
    "        # Compute MSE loss\n",
    "        mse_loss = F.mse_loss(pred, x)\n",
    "        # Compute SSIM value, which lies between 0 and 1; lower SSIM is worse, so we use (1 - SSIM) as the loss component. We need to ensure the range of the prediction is on [-1,1] so we clamp the tensor\n",
    "        pred_clamped = torch.clamp(pred, -1, 1)\n",
    "        ssim_loss = 1 - self.ssim_metric(pred_clamped, x)\n",
    "\n",
    "        return mse_loss + ssim_loss\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNArchConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A DNArch convolutional network built by stacking DNArchConvLayer layers.\n",
    "\n",
    "    This network is defined with a maximum number of layers, maximum number of channels,\n",
    "    and maximum kernel size. Both the convolution weights and the effective architecture\n",
    "    (i.e. which parts of the kernel are used) are learned via backpropagation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        max_num_hidden_channels: int,\n",
    "        max_num_hidden_layers: int,\n",
    "        max_kernel_size: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: Number of input channels.\n",
    "            max_channels: Maximum number of channels in the hidden layers.\n",
    "            out_channels: Number of output channels.\n",
    "            num_layers: Maximum number of layers in the network.\n",
    "            max_kernel_size: Maximum kernel size for each DNArchConvLayer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.L_max : int = max_num_hidden_channels + 2\n",
    "\n",
    "        # First layer: from input channels to max_channels.\n",
    "        layers.append(DNArchConvLayer(in_channels, max_channels, max_kernel_size))\n",
    "\n",
    "        # Hidden layers: from max_channels to max_channels.\n",
    "        for _ in range(max_num_hidden_layers):\n",
    "            layers.append(DNArchConvLayer(max_channels, max_channels, max_kernel_size))\n",
    "\n",
    "        # Final layer: from max_channels to desired output channels.\n",
    "        layers.append(DNArchConvLayer(max_channels, out_channels, max_kernel_size))\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        # Here we define a simple MSE loss, similar to the SIREN model.\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "    def loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the MSE loss between the prediction and the target.\n",
    "\n",
    "        Args:\n",
    "            pred: The predicted output tensor.\n",
    "            target: The ground truth tensor.\n",
    "\n",
    "        Returns:\n",
    "            The computed loss value.\n",
    "        \"\"\"\n",
    "        return self.loss_fn(pred, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DNArch Layer Definition ---\n",
    "class DNArchLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single DNArch layer that uses two shared inside networks to generate its\n",
    "    pointwise and depthwise convolution kernels. Each layer has four learnable masks:\n",
    "      - A sigmoid mask for the pointwise part.\n",
    "      - A sigmoid mask for the depthwise part.\n",
    "      - A 2D Gaussian mask (learnable, but initialized from a Gaussian).\n",
    "      - An additional sigmoid mask.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        max_kernel_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.max_kernel_size = max_kernel_size\n",
    "\n",
    "        # Define the four masks (each of shape (max_kernel_size, max_kernel_size)):\n",
    "        self.pointwise_mask = nn.Parameter(\n",
    "            torch.zeros(max_kernel_size, max_kernel_size)\n",
    "        )\n",
    "        self.depthwise_mask = nn.Parameter(\n",
    "            torch.zeros(max_kernel_size, max_kernel_size)\n",
    "        )\n",
    "        self.gaussian_mask = nn.Parameter(\n",
    "            self.create_gaussian_mask(max_kernel_size), requires_grad=True\n",
    "        )\n",
    "        self.combined_mask = nn.Parameter(torch.zeros(max_kernel_size, max_kernel_size))\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def create_gaussian_mask(self, kernel_size: int) -> torch.Tensor:\n",
    "        \"\"\"Creates an initial 2D Gaussian mask over the kernel grid.\"\"\"\n",
    "        center = kernel_size // 2\n",
    "        grid_y, grid_x = torch.meshgrid(\n",
    "            torch.arange(kernel_size), torch.arange(kernel_size), indexing=\"ij\"\n",
    "        )\n",
    "        grid_x = grid_x.float() - center\n",
    "        grid_y = grid_y.float() - center\n",
    "        sigma = kernel_size / 2.0\n",
    "        gaussian = torch.exp(-(grid_x**2 + grid_y**2) / (2 * sigma**2))\n",
    "        return gaussian\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Create coordinate grid for the kernel.\n",
    "        grid = create_kernel_grid(self.max_kernel_size).to(\n",
    "            x.device\n",
    "        )  # (K, 2) with K = max_kernel_size^2\n",
    "        K = grid.shape[0]\n",
    "\n",
    "        # Generate convolution weights via the two inside networks.\n",
    "        # Pointwise weights: (K, out_channels * in_channels) then reshaped.\n",
    "        pw_weights = self.pointwise_generator(grid)\n",
    "        pw_weights = pw_weights.view(K, self.out_channels, self.in_channels)\n",
    "\n",
    "        # Depthwise weights: (K, in_channels)\n",
    "        dw_weights = self.depthwise_generator(grid)\n",
    "        dw_weights = dw_weights.view(K, self.in_channels)\n",
    "\n",
    "        # Compute the masks.\n",
    "        mask_pw = self.sigmoid(self.pointwise_mask).view(-1)  # (K,)\n",
    "        mask_dw = self.sigmoid(self.depthwise_mask).view(-1)  # (K,)\n",
    "        mask_gauss = self.gaussian_mask.view(-1)  # (K,)\n",
    "        mask_comb = self.sigmoid(self.combined_mask).view(-1)  # (K,)\n",
    "        total_mask = mask_pw * mask_dw * mask_gauss * mask_comb  # (K,)\n",
    "\n",
    "        # Apply the mask to the generated weights.\n",
    "        pw_weights = pw_weights * total_mask.view(K, 1, 1)\n",
    "        dw_weights = dw_weights * total_mask.view(K, 1)\n",
    "\n",
    "        # Reshape weights into kernels:\n",
    "        # Pointwise kernel: (out_channels, in_channels, k, k)\n",
    "        pw_kernel = pw_weights.permute(1, 2, 0).view(\n",
    "            self.out_channels,\n",
    "            self.in_channels,\n",
    "            self.max_kernel_size,\n",
    "            self.max_kernel_size,\n",
    "        )\n",
    "        # Depthwise kernel: (in_channels, 1, k, k)\n",
    "        dw_kernel = dw_weights.permute(1, 0).view(\n",
    "            self.in_channels, 1, self.max_kernel_size, self.max_kernel_size\n",
    "        )\n",
    "\n",
    "        # Apply depthwise convolution first.\n",
    "        padding = self.max_kernel_size // 2\n",
    "        x_depth = F.conv2d(\n",
    "            x, dw_kernel, bias=None, groups=self.in_channels, padding=padding\n",
    "        )\n",
    "        # Then apply pointwise convolution.\n",
    "        out = F.conv2d(x_depth, pw_kernel, bias=None, padding=padding)\n",
    "        return out\n",
    "\n",
    "\n",
    "# --- DNArch Network Definition ---\n",
    "\n",
    "\n",
    "class DNArchConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A DNArch-style convolutional network that builds its architecture using DNArchLayer.\n",
    "    The only parameters that scale with the number of layers are the masks (4 per layer)\n",
    "    plus one additional global mask for the whole network.\n",
    "\n",
    "    The two inside networks (for pointwise and depthwise generation) are shared across layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        max_channels: int,\n",
    "        out_channels: int,\n",
    "        num_layers: int,\n",
    "        max_kernel_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Create the two inside networks (shared across layers).\n",
    "        self.pointwise_generator = PointwiseGenerator(\n",
    "            in_channels=max_channels,\n",
    "            out_channels=max_channels,\n",
    "            max_kernel_size=max_kernel_size,\n",
    "        )\n",
    "        self.depthwise_generator = DepthwiseGenerator(\n",
    "            in_channels=max_channels, max_kernel_size=max_kernel_size\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        # First layer: maps from input channels to max_channels.\n",
    "        layers.append(\n",
    "            DNArchLayer(\n",
    "                in_channels,\n",
    "                max_channels,\n",
    "                max_kernel_size,\n",
    "                self.pointwise_generator,\n",
    "                self.depthwise_generator,\n",
    "            )\n",
    "        )\n",
    "        # Hidden layers.\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(\n",
    "                DNArchLayer(\n",
    "                    max_channels,\n",
    "                    max_channels,\n",
    "                    max_kernel_size,\n",
    "                    self.pointwise_generator,\n",
    "                    self.depthwise_generator,\n",
    "                )\n",
    "            )\n",
    "        # Final layer: maps from max_channels to output channels.\n",
    "        layers.append(\n",
    "            DNArchLayer(\n",
    "                max_channels,\n",
    "                out_channels,\n",
    "                max_kernel_size,\n",
    "                self.pointwise_generator,\n",
    "                self.depthwise_generator,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        # Global mask for the whole network (a single learnable sigmoid parameter).\n",
    "        self.global_mask = nn.Parameter(torch.zeros(1))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.net(x)\n",
    "        # Apply the global mask.\n",
    "        global_mask = self.sigmoid(self.global_mask)\n",
    "        out = out * global_mask\n",
    "        return out\n",
    "\n",
    "    def loss(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        return self.loss_fn(pred, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
