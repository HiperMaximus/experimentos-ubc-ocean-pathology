{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import typeguard\n",
    "from architectures import Discriminator, Encoder, Generator\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "from torch.utils import tensorboard\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure  # MSE, PSNR\n",
    "from torchvision import datasets, transforms\n",
    "from typeguard import CollectionCheckStrategy\n",
    "\n",
    "typeguard.config.collection_check_strategy = CollectionCheckStrategy.ALL_ITEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Initialize Networks, Loss, and Optimizers\n",
    "# ------------------------------\n",
    "\n",
    "D: Discriminator = Discriminator()\n",
    "D.compile(fullgraph=True, mode=\"max-autotune\")\n",
    "G: Generator = Generator()\n",
    "G.compile(fullgraph=True, mode=\"max-autotune\")\n",
    "\n",
    "# Hyperparameters\n",
    "lr: float = 2e-4\n",
    "beta1: float = 0.01\n",
    "gamma: float = 1  # Coefficient for gradient penalty\n",
    "\n",
    "\n",
    "optimizer_D: optim.Optimizer = optim.AdamW(D.parameters(), lr=lr, betas=(beta1, 0.99))\n",
    "optimizer_G: optim.Optimizer = optim.AdamW(G.parameters(), lr=lr, betas=(beta1, 0.99))\n",
    "\n",
    "# Set the maximum number of epochs (or iterations) for annealing\n",
    "T_max = 100  # for instance, one cycle over 100 epochs\n",
    "\n",
    "# Create the cosine annealing scheduler\n",
    "scheduler_D = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_D, T_max=T_max, eta_min=1e-6)\n",
    "scheduler_G = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=T_max, eta_min=1e-6)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Data Loader for MNIST\n",
    "# ------------------------------\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # converts image to [0,1]\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # scales to [-1,1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset: datasets.MNIST = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "dataloader: DataLoader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# TensorBoard Logging\n",
    "# ------------------------------\n",
    "\n",
    "# Create a SummaryWriter instance. You can change the log_dir as needed.\n",
    "writer = SummaryWriter(log_dir=\"./runs/experiment_1\")\n",
    "global_step = 0\n",
    "\n",
    "# ------------------------------\n",
    "# Training Loop\n",
    "# ------------------------------\n",
    "\n",
    "num_epochs: int = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        batch_size: int = real_images.size(0)\n",
    "        # Symmetric target labels: real=1, fake=-1\n",
    "        # Use least squares loss (1-v)**2.\n",
    "\n",
    "        ### === Discriminator Update ===\n",
    "        # Zero-out the Discriminator optimizer\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # When generating fake images, disable gradients to avoid updating G during D's update.\n",
    "        with torch.no_grad():\n",
    "            fake_images: torch.Tensor = G(batch_size)\n",
    "\n",
    "        # Relativistic LS loss for the Discriminator with R1 + R2 zero mean gradient penalties:\n",
    "        loss_D: torch.Tensor = D.calculate_relativistic_loss(real_images, fake_images, gamma)\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        ### === Generator Update ===\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Recompute discriminator outputs with updated D:\n",
    "        # Use the same real images.\n",
    "        D_real_updated: torch.Tensor = D(real_images)\n",
    "        # Generate new fake images (fresh forward pass through G)\n",
    "        fake_images = G(batch_size)\n",
    "        D_fake_updated: torch.Tensor = D(fake_images)\n",
    "\n",
    "        # Relativistic LS loss for the Generator: we want D_fake - D_real to be close to 1\n",
    "        v = D_fake_updated - D_real_updated\n",
    "        loss_G: torch.Tensor = torch.square(1 - v).mean()\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Logging for monitoring training\n",
    "        writer.add_scalar(\"Loss/Discriminator\", loss_D.item(), global_step)\n",
    "        writer.add_scalar(\"Loss/Generator\", loss_G.item(), global_step)\n",
    "        global_step += 1\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], \"\n",
    "                f\"D Loss: {loss_D.item():.4f}, G Loss: {loss_G.item():.4f}\"\n",
    "            )\n",
    "    # Step the scheduler at the end of each epoch\n",
    "    scheduler_D.step()\n",
    "    scheduler_G.step()\n",
    "    # Update the value of gamma to the desired one\n",
    "    if gamma > 0.1:\n",
    "        gamma -= 0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
