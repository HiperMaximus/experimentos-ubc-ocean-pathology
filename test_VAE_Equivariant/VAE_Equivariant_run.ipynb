{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "#os.environ['TORCH_LOGS'] = \"+dynamo\"\n",
    "#os.environ['TORCHDYNAMO_VERBOSE'] = \"1\"\n",
    "import warnings\n",
    "# Suppress the symbolic shapes warning\n",
    "# warnings.filterwarnings(\"ignore\", message=\"xindex is not in var_ranges\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import save_image\n",
    "from vae_equivariant_architecture import D4_Equivariant_VAE\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TiledImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, image_ids, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_ids = image_ids\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        \n",
    "        # Collect all image paths for the given image IDs\n",
    "        for image_id in image_ids:\n",
    "            image_folder = os.path.join(data_dir, image_id)\n",
    "            for img_name in os.listdir(image_folder):\n",
    "                self.image_paths.append(os.path.join(image_folder, img_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `image_ids` is a list of all folder names (IDs of the original images)\n",
    "data_dir = '../UBC-Ocean-Data/Autoencoder'\n",
    "image_ids = os.listdir(data_dir)  # List of all original image IDs\n",
    "\n",
    "# Split image IDs\n",
    "train_ids, temp_ids = train_test_split(image_ids, test_size=0.3, random_state=42)\n",
    "val_ids, test_ids = train_test_split(temp_ids, test_size=2/3, random_state=42)  # 10% validation, 20% test\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TiledImageDataset(data_dir, train_ids, transform=transform)\n",
    "val_dataset = TiledImageDataset(data_dir, val_ids, transform=transform)\n",
    "test_dataset = TiledImageDataset(data_dir, test_ids, transform=transform)\n",
    "\n",
    "batch_size=2**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10% of training data for each epoch\n",
    "def get_subset_sampler(dataset, percentage=0.1):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    split = int(np.ceil(percentage * dataset_size))\n",
    "    train_indices = indices[:split]\n",
    "    \n",
    "    return SubsetRandomSampler(train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1530fd4b15064627a6f44e50d9e44c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1/20:   0%|          | 0/2162 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246206259b784568b79a69844908bb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 1/20:   0%|          | 0/1320 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.0139, Validation Loss: 0.0124, Learning Rate: 0.001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b100e6d622437eb7d64d06d3728410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2/20:   0%|          | 0/2162 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed37d323dedf4c0ea96ec420badde3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 2/20:   0%|          | 0/1320 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 0.0121, Validation Loss: 0.0124, Learning Rate: 0.001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dcadabdafd645559fb2eab14b318192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3/20:   0%|          | 0/2162 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e0295792e24e3787bb40631e60a35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 3/20:   0%|          | 0/1320 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:603] . unexpected pos 271335552 vs 271335440",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/media/max/f7de66f7-119c-4593-a8ab-02f75c636771/Max/experimentos-ubc-ocean-pathology/venv/lib/python3.8/site-packages/torch/serialization.py:652\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 652\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/media/max/f7de66f7-119c-4593-a8ab-02f75c636771/Max/experimentos-ubc-ocean-pathology/venv/lib/python3.8/site-packages/torch/serialization.py:886\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    885\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 886\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data/199: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 146\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Averaged gradient for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Save both model and optimizer states\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiled_vae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscheduler_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mval_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_samples\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg_gradients\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mavg_gradients\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoint_epoch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Save best model when validation loss improves\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "File \u001b[0;32m/media/max/f7de66f7-119c-4593-a8ab-02f75c636771/Max/experimentos-ubc-ocean-pathology/venv/lib/python3.8/site-packages/torch/serialization.py:653\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    652\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m--> 653\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n",
      "File \u001b[0;32m/media/max/f7de66f7-119c-4593-a8ab-02f75c636771/Max/experimentos-ubc-ocean-pathology/venv/lib/python3.8/site-packages/torch/serialization.py:499\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:603] . unexpected pos 271335552 vs 271335440"
     ]
    }
   ],
   "source": [
    "# Model, optimizer, and compilation\n",
    "latent_dim = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = D4_Equivariant_VAE(latent_dim)\n",
    "compiled_vae = vae.to(device)\n",
    "#compiled_vae = torch.compile(vae,fullgraph=True,dynamic=False).to(device)\n",
    "#compiled_vae.load_state_dict(torch.load('epochs/epoch_10/vae_weights_epoch_10.pth',weights_only=True))\n",
    "\n",
    "optimizer = optim.AdamW(compiled_vae.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min', \n",
    "    factor=0.9, \n",
    "    patience=1,\n",
    "    cooldown=1)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Select and save 24 images from the test set (ensure to keep this consistent across epochs)\n",
    "n=24\n",
    "fixed_batch = next(iter(DataLoader(test_dataset, batch_size=n, shuffle=True)))\n",
    "for i in range(n):\n",
    "    save_image(fixed_batch[i],fp=f'epochs/example_imgs/img_{i}.png')\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir='runs/vae_experiment')\n",
    "# Log original images to TensorBoard\n",
    "writer.add_images('Original Images', fixed_batch, 0)\n",
    "\n",
    "\n",
    "# sample 10% of data per epoch and log everything\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    sampler = get_subset_sampler(train_dataset, percentage=0.2)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    accumulated_gradients = {name: torch.zeros_like(param) for name, param in compiled_vae.named_parameters()}\n",
    "    compiled_vae.train()\n",
    "    train_loss = 0\n",
    "    num_samples=0\n",
    "    # Use tqdm to create a progress bar for the training loop\n",
    "    with tqdm(total=len(train_loader), desc=f'Training Epoch {epoch + 1}/{num_epochs}', unit='batch', position=0, leave=False) as pbar:\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            assert not torch.isnan(batch).any(), f\"Input contains NaN at epoch {epoch}\"\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            recon_batch, mu, logvar = compiled_vae(batch)\n",
    "            loss = compiled_vae.vae_loss(recon_batch, batch, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            if train_loss>len(train_loader):\n",
    "                print(f\"Error in loss. Train loss: {train_loss}, loss: {loss.item()}\")\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(compiled_vae.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # for name, param in compiled_vae.named_parameters():\n",
    "            #     if (param.grad is not None) and (param.grad.numel() > 0):\n",
    "            #         accumulated_gradients[name] += param.grad.detach().clone()\n",
    "\n",
    "            num_samples+=batch.size()[0]\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)  # Increment the progress bar by 1\n",
    "            pbar.set_postfix({'train_loss': loss.item()})  # Display current loss\n",
    "\n",
    "\n",
    "    train_loss /= num_samples\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "\n",
    "    # Create current directory if doesn't exists\n",
    "    current_dir=f'epochs/epoch_{epoch +1}/'\n",
    "    os.makedirs(current_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Validation loop\n",
    "    compiled_vae.eval()\n",
    "    val_loss = 0\n",
    "    num_samples_val=0\n",
    "    sampler = get_subset_sampler(val_dataset, percentage=1)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(val_loader), desc=f'Validation Epoch {epoch + 1}/{num_epochs}', unit='batch',position=1, leave=False) as pbar:\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                recon_batch, mu, logvar = compiled_vae(batch)\n",
    "                loss = compiled_vae.vae_loss(recon_batch, batch, mu, logvar)\n",
    "                val_loss += loss.item()\n",
    "                num_samples_val+=batch.size()[0]\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.update(1)  # Increment the progress bar by 1\n",
    "                pbar.set_postfix({'val_loss': loss.item()})  # Display current loss\n",
    "\n",
    "    val_loss /= num_samples_val\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "    # Update the learning rate scheduler based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    # The learning rate in optimizer will be updated automatically if needed\n",
    "    current_lr = scheduler.get_last_lr()[0]  # To check the current learning rate\n",
    "\n",
    "    # At the end of each epoch, save the reconstructions\n",
    "    compiled_vae.eval()\n",
    "    with torch.no_grad():\n",
    "        fixed_batch_epoch = fixed_batch.to(device)\n",
    "        mu, _ = compiled_vae.encode(fixed_batch_epoch)\n",
    "        recon_batch = compiled_vae.decode(mu)\n",
    "        recon_batch=recon_batch.cpu()\n",
    "\n",
    "        # Reverse normalization\n",
    "        recon_batch = (recon_batch*0.5)+0.5 \n",
    "        # Clamp the values to be in the range [0, 1]\n",
    "        recon_batch = torch.clamp(recon_batch, 0, 1)\n",
    "        for i in range(n):\n",
    "            save_image(recon_batch[i],fp=os.path.join(current_dir,f'img_recon_{i}.png'))\n",
    "    \n",
    "    # Log reconstructed images to TensorBoard\n",
    "    writer.add_images('Reconstructed Images', recon_batch, epoch)\n",
    "\n",
    "    # Log the model parameters and accumulated gradients to TensorBoard after the epoch\n",
    "    # avg_gradients=copy.deepcopy(accumulated_gradients)\n",
    "    # for name, param in compiled_vae.named_parameters():\n",
    "    #     # Log weights as histogram\n",
    "    #     if param.data.numel() > 0:\n",
    "    #         writer.add_histogram(f'Weights/{name}', param, global_step=epoch)\n",
    "\n",
    "    #     if (param.grad is not None) and (param.grad.numel() > 0):\n",
    "    #         # Log averaged gradients\n",
    "    #         avg_gradient = accumulated_gradients[name] / num_samples\n",
    "    #         if avg_gradient.numel() > 0:  # Check if the averaged gradient is non-empty\n",
    "    #             avg_gradients[name] = avg_gradient\n",
    "    #             writer.add_histogram(f'Gradients/{name}', avg_gradient, global_step=epoch)\n",
    "    #         else:\n",
    "    #             print(f\"Warning: Averaged gradient for {name} is empty.\")\n",
    "\n",
    "    # Save both model and optimizer states\n",
    "    torch.save({\n",
    "        'epoch': epoch+1,\n",
    "        #'model_state_dict': compiled_vae.state_dict(),\n",
    "        #'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss':val_loss,\n",
    "        'num_samples': num_samples,\n",
    "        #'avg_gradients':avg_gradients\n",
    "        'learning_rate':current_lr\n",
    "    }, os.path.join(current_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "\n",
    "    # Save best model when validation loss improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "        'epoch': epoch+1,\n",
    "        'model_state_dict': compiled_vae.state_dict(),\n",
    "        #'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss':val_loss,\n",
    "        'num_samples': num_samples,\n",
    "        #'avg_gradients':avg_gradients\n",
    "        'learning_rate':current_lr\n",
    "        }, os.path.join('epochs/best_model/', 'vae_best_model.pth'))\n",
    "\n",
    "    # Log training and validation losses\n",
    "    writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "\n",
    "    tqdm.write(f'Epoch {epoch + 1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Learning Rate: {current_lr}')\n",
    "\n",
    "\n",
    "# After training loop\n",
    "\n",
    "# Calculate test loss\n",
    "compiled_vae.eval()\n",
    "test_loss = 0\n",
    "num_samples = 0\n",
    "sampler = get_subset_sampler(test_dataset, percentage=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=sampler)\n",
    "with tqdm(total=len(test_loader), desc=f'Validation', unit='batch') as pbar:\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            recon_batch, mu, logvar = compiled_vae(batch)\n",
    "            loss = compiled_vae.vae_loss(recon_batch, batch, mu, logvar)\n",
    "            test_loss += loss.item()\n",
    "            num_samples += batch.size()[0]\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)  # Increment the progress bar by 1\n",
    "            pbar.set_postfix({'val_loss': loss.item()})  # Display current loss\n",
    "test_loss /= num_samples\n",
    "\n",
    "# Log test losses\n",
    "writer.add_scalar('Loss/Test', test_loss, epoch)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "# Print test loss\n",
    "tqdm.write(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('loss_plot.png')  # Save the plot as an image file\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        'epoch': epoch+1,\n",
    "        'model_state_dict': compiled_vae.state_dict(),\n",
    "        #'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss':val_loss,\n",
    "        'num_samples': num_samples,\n",
    "        #'avg_gradients':avg_gradients\n",
    "    }, os.path.join(current_dir, f'checkpoint_epoch_{epoch+1}.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
